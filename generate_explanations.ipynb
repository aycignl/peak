{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install shap\n"
      ],
      "metadata": {
        "id": "3R0kfeI0rZv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "931a53b2-ce0c-4118-b632-bd4bb4a73068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting shap\n",
            "  Downloading shap-0.41.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (572 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.6/572.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.3)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.65.0)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (23.1)\n",
            "Collecting slicer==0.0.7 (from shap)\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.56.4)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->shap) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2022.7.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.41.0 slicer-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbP_by2ylfAO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive, files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from IPython.display import display\n",
        "import shap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/', force_remount=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS6xlr7BlqK7",
        "outputId": "e52ceafb-b1a3-4abe-b29f-85ec9e2774af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load required files"
      ],
      "metadata": {
        "id": "UtCm_XWA8ix-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# you can change the path of the directory based on where you saved the repo.\n",
        "dataset_dir = '/peak/dataset_dir'\n",
        "pickle_dir = '/peak/pickle_dir'\n"
      ],
      "metadata": {
        "id": "qyo7AKAels5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(dataset_dir +\"/df_train.csv\")\n",
        "df_test = pd.read_csv(dataset_dir +\"/df_test.csv\")\n",
        "\n",
        "train_input = pickle.load(open(pickle_dir+\"/train_input.pickle\", \"rb\"))\n",
        "test_input = pickle.load(open(pickle_dir+\"/test_input.pickle\", \"rb\"))\n",
        "train_label = pickle.load(open(pickle_dir+\"/train_label.pickle\", \"rb\"))\n",
        "test_label = pickle.load(open(pickle_dir+\"/test_label.pickle\", \"rb\"))\n",
        "df_train_shapley = pd.read_csv(dataset_dir +\"/df_train_shapley.csv\")\n",
        "df_test_shapley = pd.read_csv(dataset_dir +\"/df_test_shapley.csv\")\n"
      ],
      "metadata": {
        "id": "IQhXA7VwXbz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics = 20\n",
        "my_list = [str(i) for i in np.arange(topics)]\n",
        "string = 'topic '\n",
        "input_columns = list(map(lambda orig_string: string + orig_string, my_list))\n",
        "\n",
        "df_train_input = pd.DataFrame(train_input, columns = input_columns)\n",
        "df_train_label = pd.DataFrame(train_label, columns = ['label'])\n",
        "\n",
        "df_test_input = pd.DataFrame(test_input, columns = input_columns)\n",
        "df_test_label = pd.DataFrame(test_label, columns = ['label'])\n"
      ],
      "metadata": {
        "id": "1ZIMn12ZtlJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_df(df_shapley, df_input):\n",
        "    df_2 = df_shapley.copy().iloc[:, :20]\n",
        "\n",
        "    columns_dict = {0: \"topic 0\", 1: \"topic 1\",\n",
        "                2: \"topic 2\", 3: \"topic 3\",\n",
        "                4: \"topic 4\", 5: \"topic 5\",\n",
        "                6: \"topic 6\", 7: \"topic 7\",\n",
        "                8: \"topic 8\", 9: \"topic 9\",\n",
        "                10: \"topic 10\", 11: \"topic 11\",\n",
        "                12: \"topic 12\", 13: \"topic 13\",\n",
        "                14: \"topic 14\", 15: \"topic 15\",\n",
        "                16: \"topic 16\", 17: \"topic 17\",\n",
        "                18: \"topic 18\", 19: \"topic 19\",\n",
        "                }\n",
        "\n",
        "    pd_idx = pd.Index(list(range(20)))\n",
        "    boolean_idx_matrix = np.zeros((len(df_shapley), 20), dtype=bool)\n",
        "    topic_list = list(df_input.apply(lambda x: x > 0, raw=True).apply(lambda x: list(df_input.columns[x.values]), axis=1))\n",
        "\n",
        "    for idx in range(len(df_2)):\n",
        "        boolean_idx_matrix[idx] = pd_idx.isin([k for k, v in columns_dict.items() if v in topic_list[idx]])\n",
        "\n",
        "    for i in range(boolean_idx_matrix.shape[0]):\n",
        "        for j in range(boolean_idx_matrix.shape[1]):\n",
        "            if (boolean_idx_matrix[i][j] == False):\n",
        "                df_2.at[i, columns_dict[j]] = 0\n",
        "\n",
        "    df_3 = df_2.abs()\n",
        "    df_4 = df_3.div(df_3.sum(axis=1), axis=0)\n",
        "\n",
        "    return df_2, df_3, df_4\n"
      ],
      "metadata": {
        "id": "vSsrJ30ts5XR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df2, train_df3, train_df4 = prep_df(df_train_shapley, df_train_input)\n",
        "test_df2, test_df3, test_df4 = prep_df(df_test_shapley, df_test_input)\n"
      ],
      "metadata": {
        "id": "2eMk5n-Is854"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexes_public = list(df_test[df_test['normalizedpublic'] == 1.0].index)\n",
        "indexes_private = list(df_test[df_test['normalizedpublic'] != 1.0].index)\n",
        "len(indexes_public), len(indexes_private)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPCJQNJtAiPP",
        "outputId": "9ebb1850-1c30-4e00-f267-99254af5e382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2500, 2500)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dominant category"
      ],
      "metadata": {
        "id": "A2HZy2f0tnvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cat_dominant(dominant_private_ub, dominant_public_ub, df_4):\n",
        "    df_dominant_private_base = df_4.loc[indexes_private]\n",
        "    df_dominant_public_base = df_4.loc[indexes_public]\n",
        "\n",
        "    df_dominant_private = df_dominant_private_base[df_dominant_private_base.max(axis=1)>=dominant_private_ub]\n",
        "    df_dominant_public = df_dominant_public_base[df_dominant_public_base.max(axis=1)>=dominant_public_ub]\n",
        "\n",
        "    df_dominant = pd.concat([df_dominant_private, df_dominant_public])\n",
        "\n",
        "    return df_dominant\n"
      ],
      "metadata": {
        "id": "nWLk-aK9AALt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Opponent category"
      ],
      "metadata": {
        "id": "ErIr3jZ8u77k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cat_opponent(opponent_private_ub, opponent_public_ub, df_4, df_2, opponent_ub_2, df_dominant, paired_ub):\n",
        "    df_5_private_base = df_4.loc[indexes_private]\n",
        "    df_5_public_base = df_4.loc[indexes_public]\n",
        "\n",
        "    df_5_private = df_5_private_base.apply(lambda x: x >= opponent_private_ub)\n",
        "    df_5_public = df_5_public_base.apply(lambda x: x >= opponent_public_ub)\n",
        "\n",
        "    df_5 = pd.concat([df_5_private, df_5_public])\n",
        "\n",
        "    df_6 = df_5.loc[df_5[(df_5.sum(axis = 1) >= 2)].index] #the number of valid elements per image >=2\n",
        "    df_6 = df_2[df_6].dropna(how='all')\n",
        "\n",
        "    df_n_n = df_6[df_6.apply(lambda x: x<0)] < 0\n",
        "    df_n_p = df_6[df_6.apply(lambda x: x>0)] > 0\n",
        "    df_n = df_n_n.sum(axis=1).to_frame('negatives')\n",
        "    df_n[\"positives\"] = df_n_p.sum(axis=1)\n",
        "\n",
        "    df_opponent = df_n[(df_n[\"negatives\"] >= opponent_ub_2)&(df_n[\"positives\"] >= opponent_ub_2)]\n",
        "    idx_intersect = list(set(df_opponent.index) & set(df_dominant.index))\n",
        "    df_opponent = df_opponent.drop(idx_intersect)\n",
        "    df_opponent = df_4.iloc[df_opponent.index]\n",
        "\n",
        "    temp_idx_list = [idx if (np.abs(df_2.iloc[idx].min()+df_2.iloc[idx].max())<paired_ub)==True else None for idx in df_opponent.index]\n",
        "    indexes_filtered_opponent = [x for x in temp_idx_list if x is not None]\n",
        "\n",
        "    return indexes_filtered_opponent, df_opponent\n"
      ],
      "metadata": {
        "id": "39N72xz4Tsqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collaborative category"
      ],
      "metadata": {
        "id": "BosnT5rhvGtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cat_collab(df_2, collaborative_private_ub, collaborative_public_ub, df_dominant, df_conflict):\n",
        "    df_7 = df_2.copy()\n",
        "    df_8 = df_7[df_7.apply(lambda x: x<0)].sum(axis=1).to_frame('negatives')\n",
        "    df_8[\"positives\"] = df_7[df_7.apply(lambda x: x>0)].sum(axis=1)\n",
        "    df_8 = df_8.abs()\n",
        "    df_8[\"summa\"] = df_8.sum(axis=1)\n",
        "    df_8[\"res_neg\"] = df_8[\"negatives\"]/df_8[\"summa\"]\n",
        "    df_8[\"res_pos\"] = df_8[\"positives\"]/df_8[\"summa\"]\n",
        "\n",
        "    df_8_private_base = df_8.loc[indexes_private]\n",
        "    df_8_public_base = df_8.loc[indexes_public]\n",
        "\n",
        "    l_neg_private = list(df_8_private_base[df_8_private_base[\"res_neg\"] >= collaborative_private_ub].index)\n",
        "    l_pos_private = list(df_8_private_base[df_8_private_base[\"res_pos\"] >= collaborative_private_ub].index)\n",
        "\n",
        "    l_neg_public = list(df_8_public_base[df_8_public_base[\"res_neg\"] >= collaborative_public_ub].index)\n",
        "    l_pos_public = list(df_8_public_base[df_8_public_base[\"res_pos\"] >= collaborative_public_ub].index)\n",
        "\n",
        "    df_collaborative_private = df_7.loc[[*l_neg_private, *l_pos_private]].sort_index()\n",
        "    df_collaborative_public = df_7.loc[[*l_neg_public, *l_pos_public]].sort_index()\n",
        "\n",
        "    df_collaborative = pd.concat([df_collaborative_private, df_collaborative_public])\n",
        "\n",
        "    idx_intersect_dom = list(set(df_collaborative.index) & set(df_dominant.index))\n",
        "    idx_intersect_con = list(set(df_collaborative.index) & set(df_conflict.index))\n",
        "    df_collaborative = df_collaborative.drop(idx_intersect_dom+idx_intersect_con)\n",
        "\n",
        "    return df_collaborative\n"
      ],
      "metadata": {
        "id": "I6PfCY_8XyqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observing misclassified images"
      ],
      "metadata": {
        "id": "ZwhqMqYcyUrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Category Dominant\n",
        "test_df_dominant = cat_dominant(0.7, 0.7, test_df4)\n",
        "indexes_cat_dominant = np.array(test_df_dominant.index)\n",
        "print(\"dominant: \", len(indexes_cat_dominant))\n",
        "\n",
        "# Category Opponent\n",
        "indexes_filtered_opponent, test_df_opponent = cat_opponent(0.2, 0.2, test_df4, test_df2, 1, test_df_dominant, 0.1)\n",
        "indexes_cat_opponent = np.array(test_df_opponent.index)\n",
        "print(\"opponent: \", len(test_df_opponent.index), len(indexes_filtered_opponent))\n",
        "\n",
        "# # Category Collaborative\n",
        "test_df_collaborative = cat_collab(test_df2, 0.8, 0.8, test_df_dominant, test_df_opponent)\n",
        "indexes_cat_collaborative = np.array(test_df_collaborative.index)\n",
        "print(\"collab: \", len(indexes_cat_collaborative))\n",
        "\n",
        "# # Category Weak\n",
        "test_df_weak = test_df2[~test_df2.index.isin(list(test_df_dominant.index)+list(test_df_opponent.index)+list(test_df_collaborative.index))]\n",
        "indexes_cat_weak = np.array(test_df_weak.index)\n",
        "print(\"weak: \", len(indexes_cat_weak))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkI2iLJdV8ho",
        "outputId": "467dd05f-2b0d-4ffa-8de5-e09be16d20ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dominant:  563\n",
            "conflicting:  848 750\n",
            "collab:  2776\n",
            "vague:  813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Explanations"
      ],
      "metadata": {
        "id": "H-xWFoCSOlr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "KvKzyRDI-XLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transform extracted_tags column into comma separated str list\n",
        "def str_to_comma_list(df):\n",
        "    tag_list_all = []\n",
        "    index_list = list(df.index)\n",
        "\n",
        "    for index in index_list:\n",
        "        tag_list_all.append([i.split(':', 1)[0] for i in df['extracted_tags'][index]][:20])\n",
        "\n",
        "    tag_list_all = [[x.lower().strip().replace('(', '').replace(')', '').replace('-', ' ') for x in y] for y in tag_list_all]\n",
        "    tag_list_all2 = [[x.replace(' ', '_') for x in y] for y in tag_list_all]\n",
        "    df['cleaned_tags_w_comma'] = tag_list_all2\n"
      ],
      "metadata": {
        "id": "AoPu00dY-WZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the tags that are in the intersection set of a selected image and topic <br>\n",
        "Warning: df_train_topic_tag: You should save the topic-tag dataframe after running NMF topic modelling. You can save the file in the pickle_dir"
      ],
      "metadata": {
        "id": "fLgvZqf--mKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "def tag_intersect_list(image_id, topic_id, df, df_train_topic_tag):\n",
        "    x = df[df.image == image_id].cleaned_tags_w_comma.item()\n",
        "\n",
        "    intersect_list = sorted(set(list(df_train_topic_tag[topic_id])) & set(ast.literal_eval(x)), key = list(df_train_topic_tag[topic_id]).index)\n",
        "    return intersect_list\n"
      ],
      "metadata": {
        "id": "SKcuPlru-jqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a dictionary in which we will store the explanation category name and topics to display in the explanation of each image."
      ],
      "metadata": {
        "id": "hxrikoLbOuZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dominant_dict = test_df_dominant.idxmax(axis=1).to_dict()\n",
        "\n",
        "opposing_dict = {index: (value1, value2) for index, value1, value2 in zip(test_df2.loc[list(test_df_conflict.index)].idxmin(axis=1).index, test_df2.loc[list(test_df_conflict.index)].idxmin(axis=1).values, test_df2.loc[list(test_df_conflict.index)].idxmax(axis=1).values)}\n",
        "\n",
        "df = train_df4.loc[list(test_df_collaborative.index)]\n",
        "collaborative_dict = df.apply(lambda row: row.nlargest(3).index.tolist(), axis=1).to_dict()\n",
        "\n",
        "top_weak = test_df4.loc[test_df_others.index].apply(lambda row: row.nlargest(3).index.tolist(), axis=1)\n",
        "\n",
        "top_negative = test_df_others.apply(lambda row: row.nlargest(3).index.tolist(), axis=1)\n",
        "top_positive = test_df_others.apply(lambda row: row.nsmallest(3).index.tolist(), axis=1)\n",
        "\n",
        "intersection_negative = {}\n",
        "intersection_positive = {}\n",
        "for idx in top_weak.index:\n",
        "    intersection_neg = set(top_weak[idx]).intersection(top_negative[idx])\n",
        "    intersection_pos = set(top_weak[idx]).intersection(top_positive[idx])\n",
        "\n",
        "    intersection_negative[idx] = list(intersection_neg)\n",
        "    intersection_positive[idx] = list(intersection_pos)\n",
        "\n",
        "\n",
        "weak_dict = {key: [intersection_negative.get(key, []), intersection_positive.get(key, [])] for key in set(intersection_negative) | set(intersection_positive)}\n",
        "\n",
        "def merge_dict_category_topic(dominant_dict, opposing_dict, collaborative_dict, weak_dict):\n",
        "    cat_top_dict = {}\n",
        "\n",
        "    for key, values in dominant_dict.items():\n",
        "        cat_top_dict[key] = ['dominant', values]\n",
        "\n",
        "    for key, values in opposing_dict.items():\n",
        "        cat_top_dict[key] = [\"opposing\", values]\n",
        "\n",
        "    for key, values in collaborative_dict.items():\n",
        "        cat_top_dict[key] = ['collaborative', values]\n",
        "\n",
        "    for key, values in weak_dict.items():\n",
        "        cat_top_dict[key] = [\"weak\", values]\n",
        "\n",
        "    return dict(sorted(cat_top_dict.items(), key=lambda item: item[0]))\n",
        "\n",
        "cat_top_dict = merge_dict_category_topic(dominant_dict, opposing_dict, collaborative_dict, weak_dict)\n"
      ],
      "metadata": {
        "id": "jvHysh6bnu6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot explanations"
      ],
      "metadata": {
        "id": "wnP4pX0_RuIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "\n",
        "def plot_explanations(idx, cat_top_dict, df):\n",
        "    category_name = cat_top_dict.get(idx)[0]\n",
        "    truth_label = ['public' if df[\"normalizedpublic\"].get(idx) == 1.0 else \"private\"][0]\n",
        "\n",
        "    image_id = df[df.index == idx][\"image\"][0]\n",
        "    # image_id = 4398397540\n",
        "\n",
        "    # Predictions\n",
        "    random_state, max_depth = 333, 13\n",
        "    classifier_rf = RandomForestClassifier(random_state = random_state, max_depth=max_depth)\n",
        "    classifier_rf_fit = classifier_rf.fit(train_input, train_label)\n",
        "    predictions = classifier_rf_fit.predict(test_input)\n",
        "    predicted_label = ['private' if predictions[idx] == 0 else \"public\"][0]\n",
        "\n",
        "    # Function to format text\n",
        "    def format_text(text, max_width):\n",
        "        words = text.split()\n",
        "        lines = []\n",
        "        current_line = []\n",
        "\n",
        "        for word in words:\n",
        "            if len(' '.join(current_line + [word])) <= max_width:\n",
        "                current_line.append(word)\n",
        "            else:\n",
        "                lines.append(' '.join(current_line))\n",
        "                current_line = [word]\n",
        "\n",
        "        if current_line:\n",
        "            lines.append(' '.join(current_line))\n",
        "\n",
        "        return '\\n'.join(lines)\n",
        "\n",
        "    # Generate text based on category\n",
        "    if category_name == \"dominant\":\n",
        "        topic = cat_top_dict.get(idx)[1]\n",
        "        # feel free to rename topics based on your topic modelling result\n",
        "        text = f\"The generated explanation for this image being assigned to the {predicted_label} class \\\n",
        "                is that it is related to the topic {topic} with these specific tags.\"\n",
        "\n",
        "        word_cloud_topics = topic\n",
        "        num_circles = len(word_cloud_topics)\n",
        "\n",
        "        if truth_label == predicted_label and truth_label == \"private\":\n",
        "            contour_colors = [\"darkviolet\"]\n",
        "        elif truth_label == predicted_label and truth_label == \"public\":\n",
        "            contour_colors = [\"darkorange\"]\n",
        "        else: # misclassification\n",
        "            contour_colors = [\"black\"]\n",
        "\n",
        "    elif category_name == \"opposing\":\n",
        "        topic_negative = cat_top_dict.get(idx)[1][0]\n",
        "        topic_positive = cat_top_dict.get(idx)[1][1]\n",
        "        # feel free to rename topics based on your topic modelling result\n",
        "        text = f\"Even though the image is related to the topic {topic_negative} with the specific tags below \\\n",
        "                which signals the {truth_label} class), it is also related to the topic {topic_positive} and \\\n",
        "                for that reason, it is classified as {predicted_label}.\"\n",
        "\n",
        "        word_cloud_topics = [topic_positive, topic_negative]\n",
        "        num_circles = len(word_cloud_topics)\n",
        "        print(\"opposing\", num_circles)\n",
        "        if truth_label == predicted_label and truth_label == \"private\":\n",
        "            contour_colors = [\"darkviolet\", \"darkorange\"]\n",
        "        elif truth_label == predicted_label and truth_label == \"public\":\n",
        "            contour_colors = [\"darkorange\", \"darkviolet\"]\n",
        "        else: # misclassification\n",
        "            contour_colors = [\"black\", \"black\"]\n",
        "\n",
        "    elif category_name == \"collaborative\":\n",
        "        topics = cat_top_dict.get(idx)[1]\n",
        "        # feel free to rename topics based on your topic modelling result\n",
        "        text = f\"The generated explanation for this image being assigned to the {predicted_label} class \\\n",
        "                is that it is related to the topics {', '.join(topics)} with these specific tags.\"\n",
        "\n",
        "        word_cloud_topics = topics\n",
        "        num_circles = len(word_cloud_topics)\n",
        "        print(\"collab\", num_circles)\n",
        "\n",
        "        if truth_label == predicted_label and truth_label == \"private\":\n",
        "            contour_colors = [\"darkviolet\", \"darkviolet\", \"darkviolet\"]\n",
        "        elif truth_label == predicted_label and truth_label == \"public\":\n",
        "            contour_colors = [\"darkorange\", \"darkorange\", \"darkorange\"]\n",
        "        else: # misclassification\n",
        "            contour_colors = [\"black\", \"black\", \"black\"]\n",
        "\n",
        "    else:\n",
        "        topic_negative = cat_top_dict.get(idx)[1][0]\n",
        "        topic_positive = cat_top_dict.get(idx)[1][1]\n",
        "        # feel free to rename topics based on your topic modelling result\n",
        "        text = f\"Even though the image is related to the topic {', '.join(topic_negative)} with the specific tags below \\\n",
        "                which signals the {truth_label} class), it is also related to the topic {', '.join(topic_positive)} and \\\n",
        "                for that reason, it is classified as {predicted_label}.\"\n",
        "\n",
        "        word_cloud_topics = [topic_positive, topic_negative]\n",
        "        num_circles = len(word_cloud_topics)\n",
        "\n",
        "        if len(topic_positive)==1 and truth_label == predicted_label and truth_label == \"private\":\n",
        "            contour_colors = [\"darkviolet\", \"darkorange\", \"darkorange\"]\n",
        "        elif len(topic_positive)==2 and truth_label == predicted_label and truth_label == \"private\":\n",
        "            contour_colors = [\"darkviolet\", \"darkviolet\", \"darkorange\"]\n",
        "        elif len(topic_positive)==3 and truth_label == predicted_label and truth_label == \"private\":\n",
        "            contour_colors = [\"darkviolet\", \"darkviolet\", \"darkviolet\"]\n",
        "        else: # misclassification\n",
        "            contour_colors = [\"black\", \"black\", \"black\"]\n",
        "\n",
        "    formatted_text = format_text(text, max_width=80)\n",
        "\n",
        "    x, y = np.ogrid[:300, :300]\n",
        "    mask = (x - 150) ** 2 + (y - 150) ** 2 > 130 ** 2\n",
        "    mask = 255 * mask.astype(int)\n",
        "\n",
        "    contour_color = \"darkviolet\"  # or darkorange\n",
        "\n",
        "    # Calculate the number of columns based on the number of circles\n",
        "    num_circles = len(word_cloud_topics)\n",
        "    # Create the subplots grid\n",
        "    num_cols = num_circles + 1  # One extra column for the text\n",
        "    fig, axs = plt.subplots(1, num_cols, figsize=(5 * num_cols, 10))\n",
        "\n",
        "    # Plot the text at the top\n",
        "    axs[0].text(0.5, 0.5, formatted_text, ha='center', va='center', fontsize=12)\n",
        "    axs[0].axis(\"off\")\n",
        "\n",
        "    # Create WordClouds and plot circles in the remaining columns\n",
        "    wordclouds = []  # List to hold the generated WordCloud objects\n",
        "\n",
        "    for i in range(num_circles):\n",
        "        wordcloud = WordCloud(width=500, height=300, margin=3, prefer_horizontal=0.7, scale=1,\n",
        "                              background_color=\"white\", mask=mask, contour_width=0.1,\n",
        "                              contour_color=contour_colors[i], relative_scaling=0)\n",
        "\n",
        "        x = df[df.image == image_id].cleaned_tags_w_comma.item()\n",
        "        print(x)\n",
        "        topic_id = int(word_cloud_topics[i].split()[-1])\n",
        "        tags = sorted(set(list(df_train_topic_tag[topic_id])) & set(ast.literal_eval(x)), key = list(df_train_topic_tag[topic_id]).index)\n",
        "        tags = \" \".join(tags)\n",
        "        print(tags)\n",
        "\n",
        "        wordcloud.generate(tags)\n",
        "\n",
        "        axs[i + 1].imshow(wordcloud)\n",
        "        axs[i + 1].title.set_text(word_cloud_topics[i])\n",
        "        axs[i + 1].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "_6RQu2UjRshv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
